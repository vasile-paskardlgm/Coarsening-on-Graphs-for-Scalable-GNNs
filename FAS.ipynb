{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modification-friendly implementation of FAS via jupyter notebook\n",
        "\n",
        "Jupyter notebook makes more convenient modification and active interaction coding than a monotonous python compiler."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zATM4uEsBlg-"
      },
      "source": [
        "## Environmental Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb3QntNUNOzP",
        "outputId": "7929d4b1-5ef1-44bd-eaf3-4fde79e605af"
      },
      "outputs": [],
      "source": [
        "%pip install torch torchvision torchaudio\n",
        "%pip install torch-geometric\n",
        "%pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv\n",
        "%pip install ogb\n",
        "%pip install PyMetis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ1pHukjppYz",
        "outputId": "ea53c2c6-f784-42e9-eda5-e210ea692a0c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PATH = os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivRiU6xMBryF"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "- This section focuses on generating the coarsened data, including coarsened feature, coarsed edge index, and coarsening matrix. The generated data will be stored in the `preprocessed` folder.\n",
        "- You must run this before initializing the training.\n",
        "- Two sequential coarsening procedures, with 0.1 coarsening ratio of each, are provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_nRZbIgEo1q"
      },
      "outputs": [],
      "source": [
        "## METIS, cpu-based implementation.\n",
        "\n",
        "import torch\n",
        "import pymetis\n",
        "\n",
        "def metis_coarsen_normalized(edge_index: torch.Tensor, r: float):\n",
        "    \"\"\"\n",
        "    Partitions an undirected, unweighted graph using METIS (via pymetis), constructs the\n",
        "    normalized partition matrix (C) and the coarsened graph's edge index computed as C A C^T.\n",
        "\n",
        "    The partition matrix is normalized so that for each supernode (cluster), the nonzero\n",
        "    entry is divided by sqrt(number of nodes in the cluster).\n",
        "\n",
        "    Args:\n",
        "        edge_index (torch.Tensor): A 2 x E tensor (CPU) where each column [i, j] represents an\n",
        "                                   undirected edge between nodes i and j.\n",
        "        r (float): Coarsening ratio; the number of clusters is computed as k = int(n * r), clamped to [1, n].\n",
        "\n",
        "    Returns:\n",
        "        partition_matrix (torch.Tensor): A (n x k) normalized binary matrix where each row is a one-hot\n",
        "                                         vector indicating the cluster assignment of that node.\n",
        "        coarse_edge_index (torch.Tensor): A 2 x E_coarse tensor representing the edges of the coarsened graph\n",
        "                                            (constructed as C A C^T) with no duplicate edges and no self-loops.\n",
        "    \"\"\"\n",
        "    # Work on CPU\n",
        "    edge_index_cpu = edge_index.cpu()\n",
        "\n",
        "    # Determine the number of nodes (assuming nodes are 0-indexed)\n",
        "    n = int(edge_index_cpu.max().item() + 1)\n",
        "    # Compute number of clusters (partitions)\n",
        "    k = max(1, int(n * r))\n",
        "    k = min(k, n)\n",
        "\n",
        "    # Build the adjacency list for pymetis.\n",
        "    src = edge_index_cpu[0].tolist()\n",
        "    dst = edge_index_cpu[1].tolist()\n",
        "    adjacency = [set() for _ in range(n)]\n",
        "    for i, j in zip(src, dst):\n",
        "        adjacency[i].add(j)\n",
        "        adjacency[j].add(i)  # Ensure undirectedness.\n",
        "    adjacency = [list(neighbors) for neighbors in adjacency]\n",
        "\n",
        "    # Partition the graph using PyMetis.\n",
        "    # pymetis.part_graph returns (edgecut, parts) where parts is a list of cluster assignments.\n",
        "    _, parts = pymetis.part_graph(k, adjacency)\n",
        "\n",
        "    # Build the partition matrix (n x k) with one-hot encoding.\n",
        "    partition_matrix = torch.zeros(n, k, dtype=torch.float32)\n",
        "    parts_tensor = torch.tensor(parts, dtype=torch.long)\n",
        "    partition_matrix.scatter_(1, parts_tensor.unsqueeze(1), 1.0)\n",
        "\n",
        "    # Normalize the partition matrix:\n",
        "    # For each cluster j, divide the corresponding column by sqrt(number of nodes in cluster j)\n",
        "    cluster_counts = partition_matrix.sum(dim=0) + 1  # shape: (k,)\n",
        "    norm_factors = torch.sqrt(cluster_counts)\n",
        "    partition_matrix = partition_matrix / norm_factors.unsqueeze(0)\n",
        "\n",
        "    # --- Construct the coarsened graph's edge index ---\n",
        "    # \"Lift\" each original edge (i,j) to (parts[i], parts[j])\n",
        "    coarse_u = parts_tensor[edge_index_cpu[0]]\n",
        "    coarse_v = parts_tensor[edge_index_cpu[1]]\n",
        "\n",
        "    # Remove self-loops (intra-cluster edges)\n",
        "    mask = coarse_u != coarse_v\n",
        "    coarse_u = coarse_u[mask]\n",
        "    coarse_v = coarse_v[mask]\n",
        "\n",
        "    # Sort each edge pair so that (u,v) and (v,u) are treated as the same edge.\n",
        "    row = torch.min(coarse_u, coarse_v)\n",
        "    col = torch.max(coarse_u, coarse_v)\n",
        "    coarse_edges = torch.stack((row, col), dim=0)\n",
        "\n",
        "    # Remove duplicate edges.\n",
        "    coarse_edge_index = torch.unique(coarse_edges, dim=1)\n",
        "\n",
        "    return partition_matrix, coarse_edge_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMBB99COYjX4",
        "outputId": "a4eea2ab-0b65-4113-c551-d235f4bbcf85"
      },
      "outputs": [],
      "source": [
        "from dataset import load_nc_dataset\n",
        "\n",
        "dataset = load_nc_dataset('ogbn-arxiv')\n",
        "node_feat = dataset.graph['node_feat']\n",
        "edge_index = dataset.graph['edge_index']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ_rZY4DgYTf"
      },
      "source": [
        "### First step coarsening: G -> G1, r = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NlFJhLvgQWf"
      },
      "outputs": [],
      "source": [
        "c1, g1 = metis_coarsen_normalized(edge_index, 0.1)\n",
        "\n",
        "torch.save(c1, PATH+'/preprocessed/C1.pt')\n",
        "torch.save(g1, PATH+'/preprocessed/G1.pt')\n",
        "\n",
        "node_feat = c1.T @ node_feat\n",
        "torch.save(node_feat, PATH+'/preprocessed/X1.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHyomYnXiPdf"
      },
      "source": [
        "### Second step coarsening: G1 -> G2, r = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVFDtZIniiOH"
      },
      "outputs": [],
      "source": [
        "c2, g2 = metis_coarsen_normalized(g1, 0.1)\n",
        "\n",
        "torch.save(c1 @ c2, PATH+'/preprocessed/C2C1.pt')\n",
        "torch.save(g2, PATH+'/preprocessed/G2.pt')\n",
        "\n",
        "node_feat = c2.T @ node_feat\n",
        "torch.save(node_feat, PATH+'/preprocessed/X2.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp0OkCsXI85d"
      },
      "source": [
        "## FAS Training\n",
        "\n",
        "- Main training code for FAS. Please make sure you have generate the data.\n",
        "- If you want to do the full graph training, please only run the **Level 1 training** with setting the `epoch` to be 500, then run the testing code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDG-TL8oJAIX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, hidden_channels=256, num_layers=3,\n",
        "                 dropout=0.5):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GCNConv(in_channels, hidden_channels, cached=False))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(\n",
        "                GCNConv(hidden_channels, hidden_channels, cached=False))\n",
        "        self.convs.append(GCNConv(hidden_channels, out_channels, cached=False))\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, adj_t)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, adj_t)\n",
        "        return x.log_softmax(dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7YUNMSmL41p",
        "outputId": "94d4d298-b955-47fa-a560-d7ec67170d5a"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "def one_hot(x, class_count):\n",
        "    return torch.eye(class_count, device=x.device)[x, :]\n",
        "\n",
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def create_mask(indices, n):\n",
        "    # Create a tensor of zeros with length n\n",
        "    mask = torch.zeros(n, dtype=torch.float, device=indices.device)\n",
        "\n",
        "    # Set the positions of the training indices to 1\n",
        "    mask[indices] = 1.0\n",
        "\n",
        "    return mask\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(device)\n",
        "\n",
        "## loss functions\n",
        "loss_nll = nn.NLLLoss()\n",
        "loss_kl = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "from dataset import load_nc_dataset\n",
        "dataset = load_nc_dataset('ogbn-arxiv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVdUDw5RO_Xw"
      },
      "outputs": [],
      "source": [
        "set_seed(2703)\n",
        "\n",
        "model = GCN(dataset.num_features, dataset.num_classes).to(device)\n",
        "model.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "splits = dataset.get_idx_split()\n",
        "train_idx = splits['train'].to(device)\n",
        "test_idx = splits['test'].to(device)\n",
        "\n",
        "x0 = dataset.graph['node_feat'].to(device)\n",
        "idx0 = dataset.graph['edge_index'].to(device)\n",
        "y0 = dataset.label.to(device)\n",
        "\n",
        "complem_idx = torch.arange(x0.shape[0]).to(device)\n",
        "complem_idx = complem_idx[~torch.isin(complem_idx, train_idx)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1ydNyfaOm61"
      },
      "source": [
        "### Level 1 training on the original graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP5pzemPPuli"
      },
      "outputs": [],
      "source": [
        "x = x0.clone()\n",
        "idx = idx0.clone()\n",
        "y = y0.clone()\n",
        "\n",
        "for _ in range(50):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  out = model(x, idx)[train_idx]\n",
        "  loss = loss_nll(out, y[train_idx])\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yADlEQYXWlD"
      },
      "source": [
        "### Level 2 training on G1 (r=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IGTHejS3j5n",
        "outputId": "7d1f46e4-b9dd-422c-b315-c46503a7b548"
      },
      "outputs": [],
      "source": [
        "c = torch.load(PATH+'/preprocessed/C1.pt').to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiuECy3bW_dk",
        "outputId": "4b6219c4-78bb-40ba-c86d-6ed94dc69586"
      },
      "outputs": [],
      "source": [
        "## data update\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "\n",
        "  ## τ-correction\n",
        "  residual = c.T @ model(x0, idx0)\n",
        "\n",
        "  x = torch.load(PATH+'/preprocessed/X1.pt').to(device)\n",
        "  idx = torch.load(PATH+'/preprocessed/G1.pt').to(device)\n",
        "\n",
        "  residual = model(x, idx) - residual\n",
        "\n",
        "  ## update the label on coarsened graph\n",
        "  y = y0.clone()\n",
        "  y[complem_idx] = model(x0, idx0).argmax(dim=1).long()[complem_idx] ## incorporate the inferred label\n",
        "  y = one_hot(y, dataset.num_classes)\n",
        "  y = c.T @ y\n",
        "  y = y + residual ## incorporate τ-correction\n",
        "  y = y.log_softmax(dim=-1)\n",
        "  y = y.detach()\n",
        "\n",
        "## training\n",
        "for _ in range(100):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  out = model(x, idx)\n",
        "  loss = loss_nll(out, y.argmax(dim=1).long())\n",
        "  # loss = loss_nll(out[mask],y[mask].argmax(dim=1).long()) \n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "r'''\n",
        "# If you want to discard the inferred label and dismiss all non-training nodes...\n",
        "\n",
        "## data update\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "\n",
        "  ## τ-correction\n",
        "  residual = c.T @ model(x0, idx0)\n",
        "\n",
        "  x = torch.load(PATH+'/preprocessed/X1.pt').to(device)\n",
        "  idx = torch.load(PATH+'/preprocessed/G1.pt').to(device)\n",
        "\n",
        "  residual = model(x, idx) - residual\n",
        "\n",
        "  y = y0.clone()\n",
        "  y = one_hot(y, dataset.num_classes)\n",
        "  y[complem_idx, :] = torch.zeros_like(y[complem_idx, :])\n",
        "  y = c.T @ y\n",
        "  y = y + residual\n",
        "  y = y.log_softmax(dim=-1)\n",
        "  y = y.detach()\n",
        "\n",
        "  mask = torch.nonzero(y.any(dim=1)).squeeze()\n",
        "\n",
        "## training\n",
        "for _ in range(100):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  out = model(x, idx)\n",
        "  loss = loss_nll(out[mask],y[mask].argmax(dim=1).long()) \n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si5YylxQgtC4"
      },
      "source": [
        "### Level 3 training on G2 (r=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U08C3PP95KV7",
        "outputId": "6c85588d-18a1-4e04-efd3-08ec0afc8815"
      },
      "outputs": [],
      "source": [
        "c = torch.load(PATH+'/preprocessed/C2C1.pt').to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43ly-_yuhpye",
        "outputId": "aa45d272-5196-4e37-a2b4-ef65994ee991"
      },
      "outputs": [],
      "source": [
        "## data update\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "\n",
        "  ## τ-correction\n",
        "  residual = c.T @ model(x0, idx0)\n",
        "\n",
        "  x = torch.load(PATH+'/preprocessed/X2.pt').to(device)\n",
        "  idx = torch.load(PATH+'/preprocessed/G2.pt').to(device)\n",
        "\n",
        "  residual = model(x, idx) - residual\n",
        "\n",
        "  ## update the label on coarsened graph\n",
        "  y = y0.clone()\n",
        "  y[complem_idx] = model(x0, idx0).argmax(dim=1).long()[complem_idx] ## incorporate the inferred label\n",
        "  y = one_hot(y, dataset.num_classes)\n",
        "  y = c.T @ y\n",
        "  y = y + residual ## incorporate τ-correction\n",
        "  y = y.log_softmax(dim=-1)\n",
        "  y = y.detach()\n",
        "\n",
        "## training\n",
        "for _ in range(200):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  out = model(x, idx)\n",
        "  loss = loss_nll(out, y.argmax(dim=1).long())\n",
        "  # loss = loss_nll(out[mask],y[mask].argmax(dim=1).long()) \n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "r'''\n",
        "# If you want to discard the inferred label and dismiss all non-training nodes...\n",
        "\n",
        "## data update\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "\n",
        "  ## τ-correction\n",
        "  residual = c.T @ model(x0, idx0)\n",
        "\n",
        "  x = torch.load(PATH+'/preprocessed/X2.pt').to(device)\n",
        "  idx = torch.load(PATH+'/preprocessed/G2.pt').to(device)\n",
        "\n",
        "  residual = model(x, idx) - residual\n",
        "\n",
        "  y = y0.clone()\n",
        "  y = one_hot(y, dataset.num_classes)\n",
        "  y[complem_idx, :] = torch.zeros_like(y[complem_idx, :])\n",
        "  y = c.T @ y\n",
        "  y = y + residual\n",
        "  y = y.log_softmax(dim=-1)\n",
        "  y = y.detach()\n",
        "\n",
        "  mask = torch.nonzero(y.any(dim=1)).squeeze()\n",
        "\n",
        "## training\n",
        "for _ in range(100):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  out = model(x, idx)\n",
        "  loss = loss_nll(out[mask],y[mask].argmax(dim=1).long()) \n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mNhEgWPkUmU"
      },
      "source": [
        "### Level 2 training on G1 (r=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWXyBJjN5NHT",
        "outputId": "35bb9457-fabf-4da6-dcda-9ecb10bbb7e0"
      },
      "outputs": [],
      "source": [
        "c = torch.load(PATH+'/preprocessed/C1.pt').to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8uc3UQRkXTC",
        "outputId": "446af905-a288-4a35-a9e6-af68fad6b5f3"
      },
      "outputs": [],
      "source": [
        "# Inferred labels are included to construct the training labels\n",
        "\n",
        "## data update\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "\n",
        "  ## τ-correction\n",
        "  residual = c.T @ model(x0, idx0)\n",
        "\n",
        "  x = torch.load(PATH+'/preprocessed/X1.pt').to(device)\n",
        "  idx = torch.load(PATH+'/preprocessed/G1.pt').to(device)\n",
        "\n",
        "  residual = model(x, idx) - residual\n",
        "\n",
        "  ## update the label on coarsened graph\n",
        "  y = y0.clone()\n",
        "  y[complem_idx] = model(x0, idx0).argmax(dim=1).long()[complem_idx] ## incorporate the inferred label\n",
        "  y = one_hot(y, dataset.num_classes)\n",
        "  y = c.T @ y\n",
        "  y = y + residual ## incorporate τ-correction\n",
        "  y = y.log_softmax(dim=-1)\n",
        "  y = y.detach()\n",
        "\n",
        "## training\n",
        "for _ in range(100):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  out = model(x, idx)\n",
        "  loss = loss_nll(out, y.argmax(dim=1).long())\n",
        "  # loss = loss_nll(out[mask],y[mask].argmax(dim=1).long()) \n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "r'''\n",
        "# If you want to discard the inferred label and dismiss all non-training nodes...\n",
        "\n",
        "## data update\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "\n",
        "  ## τ-correction\n",
        "  residual = c.T @ model(x0, idx0)\n",
        "\n",
        "  x = torch.load(PATH+'/preprocessed/X1.pt').to(device)\n",
        "  idx = torch.load(PATH+'/preprocessed/G1.pt').to(device)\n",
        "\n",
        "  residual = model(x, idx) - residual\n",
        "\n",
        "  y = y0.clone()\n",
        "  y = one_hot(y, dataset.num_classes)\n",
        "  y[complem_idx, :] = torch.zeros_like(y[complem_idx, :]) ## enforce zero in non-training nodes\n",
        "  y = c.T @ y\n",
        "  y = y + residual\n",
        "  y = y.log_softmax(dim=-1)\n",
        "  y = y.detach()\n",
        "\n",
        "  mask = torch.nonzero(y.any(dim=1)).squeeze() ## dismiss the zero vectors, only non-zero vectors are considered as training set\n",
        "\n",
        "## training\n",
        "for _ in range(100):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  out = model(x, idx)\n",
        "  loss = loss_nll(out[mask],y[mask].argmax(dim=1).long())\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFcT-BSilL5J"
      },
      "source": [
        "### Level 1 training on the original graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks2JTwEkl_d7"
      },
      "outputs": [],
      "source": [
        "x = x0.clone()\n",
        "idx = idx0.clone()\n",
        "y = y0.clone()\n",
        "\n",
        "## training\n",
        "\n",
        "for _ in range(50):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  out = model(x, idx)[train_idx]\n",
        "  loss = loss_nll(out, y[train_idx])\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC2-yICMnRm4"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ2unDsgnTUu",
        "outputId": "68e22f0b-a826-4403-ea3c-eb5097c9c758"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "\n",
        "  pred = model(x0.clone(), idx0.clone()).max(1)[1]\n",
        "  test_acc = int(pred[test_idx].eq(y0[test_idx]).sum().item()) / int(test_idx.shape[0])\n",
        "  print(f'The testing accuracy is {test_acc}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ivRiU6xMBryF",
        "0yADlEQYXWlD",
        "si5YylxQgtC4",
        "4mNhEgWPkUmU"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
